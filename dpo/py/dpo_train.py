import os
import torch
from datasets import load_dataset
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TrainerCallback
from trl import DPOTrainer, DPOConfig

# 1. ç¯å¢ƒä¸è·¯å¾„
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" 
sft_model_path = "../../post_model__after_sft"
dpo_data_path = "../dataset/train.json" # ä½ çš„4000æ¡æ•°æ®

# 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
tokenizer = GPT2TokenizerFast.from_pretrained(sft_model_path)
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained(sft_model_path)
ref_model = GPT2LMHeadModel.from_pretrained(sft_model_path)

# 3. è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼šç”¨äºå®æ—¶è§‚å¯Ÿæ¨¡å‹â€œåŒ»æœ¯â€çš„å˜åŒ–
class VisualFeedbackCallback(TrainerCallback):
    def on_step_end(self, args, state, control, model=None, **kwargs):
        # æ¯ 100 æ­¥æµ‹è¯•ä¸€æ¬¡æ¨¡å‹
        if state.global_step % 100 == 0 and state.global_step > 0:
            model.eval()
            test_prompt = "é—®ï¼šå£è…”æºƒç–¡æ€ä¹ˆåŠï¼Ÿ ç­”ï¼š"
            inputs = tokenizer(test_prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.7)
            print(f"\n\næ­¥æ•° {state.global_step} å®æ—¶æµ‹è¯•åé¦ˆï¼š")
            print(tokenizer.decode(outputs[0], skip_special_tokens=True))
            print("-" * 30)
            model.train()

# 4. æ•°æ®å¤„ç†
dataset = load_dataset("json", data_files=dpo_data_path, split="train")
def format_dpo(example):
    return {
        "prompt": f"é—®ï¼š{example['question']} ç­”ï¼š",
        "chosen": example['response_chosen'],
        "rejected": example['response_rejected']
    }
dpo_dataset = dataset.map(format_dpo)

# 5. è®¡ç®—ä¿å­˜æ­¥æ•° (æ¯10%ä¿å­˜ä¸€æ¬¡)
# æ€»æ­¥æ•° = (æ ·æœ¬æ•° / BatchSize / æ¢¯åº¦ç´¯ç§¯) * Epochs
batch_size = 2
grad_acc = 4
total_steps = (len(dpo_dataset) // (batch_size * grad_acc)) * 1 
save_steps = max(1, total_steps // 10) 

# 6. è®­ç»ƒå‚æ•°
training_args = DPOConfig(
    output_dir="./med_dpo_checkpoints",
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=grad_acc,
    max_length=512,
    max_prompt_length=256,
    learning_rate=5e-7,               # DPOå­¦ä¹ ç‡æä½ï¼Œé˜²æ­¢åˆ·åè„‘å­
    num_train_epochs=1,               # 4000æ¡è·‘1éè¶³çŸ£
    logging_steps=10,                 # é¢‘ç¹æ‰“å°Lossåé¦ˆ
    save_steps=save_steps,            # è‡ªåŠ¨è®¡ç®—çš„10%æ­¥æ•°
    eval_strategy="no",
    use_cpu=True,
    remove_unused_columns=False,
    report_to="none",
    beta=0.1                          # DPO çš„ beta å‚æ•°
)

# 7. å¯åŠ¨ DPO
dpo_trainer = DPOTrainer(
    model,
    ref_model,
    args=training_args,
    train_dataset=dpo_dataset,
    processing_class=tokenizer,
    callbacks=[VisualFeedbackCallback()] # æ³¨å…¥å®æ—¶åé¦ˆå›è°ƒ
)

print(f"ğŸš€ DPOå¯åŠ¨ï¼é¢„ä¼°æ€»æ­¥æ•°: {total_steps}, æ¯ {save_steps} æ­¥ä¿å­˜ä¸€æ¬¡ã€‚")
dpo_trainer.train()