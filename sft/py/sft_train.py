import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # ç¦ç”¨ CUDA
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1" # å…è®¸å›é€€åˆ° CPU
import torch

# å¼ºåˆ¶å°†é»˜è®¤è®¾å¤‡è®¾ä¸º CPU
if torch.backends.mps.is_available():
    # è¿™ä¸€æ­¥æœ€å…³é”®ï¼šæ¬ºéª—ç¨‹åºï¼Œè®©å®ƒä»¥ä¸ºæ²¡æœ‰ MPS
    torch.set_default_device('cpu')

from datasets import load_dataset
from transformers import (
    GPT2LMHeadModel, 
    GPT2TokenizerFast, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)

# 1. è·¯å¾„è®¾ç½®
base_model_path = "../../base_model__after_pretraining"  # æŒ‡å‘é¢„è®­ç»ƒå¥½çš„åŸºåº§æ¨¡å‹æ–‡ä»¶å¤¹
sft_data_path = "../dataset/extracted_5000.json" # å»ºè®®ä½ å…ˆæå– 5000 æ¡å­˜æˆè¿™ä¸ªæ–‡ä»¶
output_sft_path = "./med_sft_model"

# 2. åŠ è½½åŸºåº§æ¨¡å‹å’Œåˆ†è¯å™¨
print("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒåŸºåº§æ¨¡å‹...")
tokenizer = GPT2TokenizerFast.from_pretrained(base_model_path)
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained(base_model_path)

# 3. æ•°æ®é¢„å¤„ç†å‡½æ•° (é’ˆå¯¹ä½ çš„ JSON å­—æ®µè¿›è¡Œäº†é€‚é…)
def sft_tokenize_function(examples):
    texts = []
    for inst, inp, out in zip(examples["instruction"], examples["input"], examples["output"]):
        # å¦‚æœæœ‰é¢å¤–çš„ input ä¿¡æ¯å°±å¸¦ä¸Šï¼Œæ²¡æœ‰å°±ç›´æ¥æ¥æŒ‡ä»¤
        user_prompt = f"{inst} {inp}".strip()
        # æ„é€ å¯¹è¯æ¨¡ç‰ˆ
        full_text = f"é—®ï¼š{user_prompt} ç­”ï¼š{out}{tokenizer.eos_token}"
        texts.append(full_text)
    
    return tokenizer(texts, truncation=True, max_length=512, padding=False)

# 4. åŠ è½½æ•°æ®
print("æ­£åœ¨åŠ è½½ SFT æ•°æ®é›†...")
# æ³¨æ„ï¼šå¦‚æœä½ çš„æ–‡ä»¶æ¯è¡Œæ˜¯ä¸€ä¸ª JSONï¼Œç”¨ "json" åŠ è½½å³å¯
dataset = load_dataset("json", data_files=sft_data_path, split="train")

# å†æ¬¡å»ºè®®ï¼šå³ä½¿æ–‡ä»¶å¾ˆå¤§ï¼Œæˆ‘ä»¬ä¹Ÿåªé€‰ 5000 æ¡ï¼Œå¯¹ 8M æ¨¡å‹æœ€å‹å¥½
if len(dataset) > 5000:
    dataset = dataset.shuffle(seed=42).select(range(5000))

tokenized_dataset = dataset.map(
    sft_tokenize_function, 
    batched=True, 
    remove_columns=dataset.column_names
)

# 5. SFT è®­ç»ƒå‚æ•° (ä¸“ä¸º Intel Mac CPU ä¼˜åŒ–)
training_args = TrainingArguments(
    use_cpu=True,  # æ˜¾å¼å£°æ˜åªç”¨ CPU
    output_dir=output_sft_path,
    num_train_epochs=3,              # è·‘ 3 éï¼Œè®©æ¨¡å‹å­¦ä¼šâ€œé—®ç­”â€çš„è§„çŸ©
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,    # å˜ç›¸å¢åŠ  Batch Size åˆ° 8ï¼Œè®­ç»ƒæ›´å¹³ç¨³
    save_steps=200,
    logging_steps=50,
    learning_rate=3e-5,               # SFT çš„å­¦ä¹ ç‡è¦æ¯”é¢„è®­ç»ƒï¼ˆ5e-4ï¼‰å°å¾—å¤š
    weight_decay=0.01,
    fp16=False,
    push_to_hub=False,
    report_to="none"
)

# 6. å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
)

print("ğŸš€ å¯åŠ¨åŒ»ç”Ÿâ€œè§„çŸ©â€ç‰¹è®­ (SFT)...")
trainer.train()

# 7. æœ€ç»ˆä¿å­˜
trainer.save_model(output_sft_path)
tokenizer.save_pretrained(output_sft_path)
print(f"âœ… ç‰¹è®­å®Œæˆï¼åŒ»ç”Ÿç°åœ¨å¯ä»¥æ­£å¼é—®è¯Šäº†ï¼Œæ¨¡å‹ä¿å­˜åœ¨: {output_sft_path}")