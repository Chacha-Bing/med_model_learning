from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import time

app = FastAPI()

MODELSTYLE = {
  "BASE": "Base (é€šè¿‡é¢„è®­ç»ƒ)",
  "SFT": "SFT (é€šè¿‡æŒ‡ä»¤å¾®è°ƒ)",
  "DPO": "DPO (é€šè¿‡åå¥½å¯¹é½)"
}

models = {
    MODELSTYLE["BASE"]: "./base_model__after_pretraining", 
    MODELSTYLE["SFT"]: "./post_model__after_sft",
    MODELSTYLE["DPO"]: "./final_model__after_dpo" # ä½¿ç”¨ä½ æ•ˆæœæœ€å¥½çš„é‚£ä¸ª
}
def generate_response(model_path, prompt, tokenizer):
    print(f"\n[æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path} ...]")
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()

    # æ„å»ºå¯¹è¯æ¨¡ç‰ˆ (ç¡®ä¿ä¸ SFT/DPO è®­ç»ƒæ—¶ä¸€è‡´)
    full_prompt = f"é—®ï¼š{prompt} ç­”ï¼š"
    inputs = tokenizer(full_prompt, return_tensors="pt")
    
    # å¼€å§‹ç”Ÿæˆ
    start_time = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,        # æ”¾å®½åˆ° 500 tokens
            do_sample=True,
            temperature=0.7,           # ä¿æŒé€‚åº¦éšæœºæ€§
            top_p=0.9,
            repetition_penalty=1.2,    # ç¨å¾®åŠ å¤§æƒ©ç½šï¼Œç¼“è§£å¤è¯»
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    
    duration = time.time() - start_time
    # è§£ç å¹¶æˆªæ–­ prompt éƒ¨åˆ†
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_text.replace(full_prompt, "").strip()
    
    # é‡Šæ”¾æ¨¡å‹å†…å­˜ï¼Œé˜²æ­¢ Mac å¡æ­»
    del model
    return response, duration

class ChatRequest(BaseModel):
    prompt: str
    model_called: Optional[str] = models[MODELSTYLE["BASE"]]

@app.post("/generate")
async def generate(request: ChatRequest):
  # åªéœ€è¦åŠ è½½ä¸€æ¬¡åˆ†è¯å™¨
  tokenizer = GPT2TokenizerFast.from_pretrained(models["SFT (é€šè¿‡æŒ‡ä»¤å¾®è°ƒ)"])
  tokenizer.pad_token = tokenizer.eos_token
  print("ğŸ¥ åŒ»ç–—å°æ¨¡å‹å¼€å§‹å·¥ä½œ ğŸ¥")
  
  results = {}
  try:
      response, dt = generate_response(request.model_called, request.prompt, tokenizer)
      results = (response, dt)
  except Exception as e:
      results = (f"åŠ è½½å¤±è´¥: {str(e)}", 0)

  return {"content": results[0], "duration": results[1]}
  # # è¿™é‡Œæ˜¯ç®€å•çš„åŒæ­¥è°ƒç”¨ï¼Œè¿›é˜¶å¯ä»¥åšæˆæµå¼è¾“å‡º (Streaming)
  # inputs = tokenizer(request.prompt, return_tensors="pt").to("cuda")
  # outputs = model.generate(**inputs, max_new_tokens=200)
  # response = tokenizer.decode(outputs[0], skip_special_tokens=True)
  # return {"content": response}
  
  
if __name__ == "__main__":
  import uvicorn
  # æ‰“å°ä¸€å¥è¯ï¼Œç¡®ä¿ä½ çœ‹åˆ°å®ƒå¼€å§‹äº†
  print("ğŸš€ åŒ»ç–— AI æ¨¡å‹æ¨ç†æœåŠ¡æ­£åœ¨å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 8000...")
  uvicorn.run(app, host="0.0.0.0", port=8000)