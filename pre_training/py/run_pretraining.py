import os
os.environ["CUDA_VISIBLE_DEVICES"] = "" # å±è”½è‹±ä¼Ÿè¾¾æ˜¾å¡
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0" # ç¦ç”¨è‹¹æœ MPS æ˜¾å­˜ä¸Šé™

import torch
from transformers import (
    GPT2Config, 
    GPT2LMHeadModel, 
    GPT2TokenizerFast, 
    DataCollatorForLanguageModeling, 
    Trainer, 
    TrainingArguments,
    TrainerCallback
)
from datasets import load_dataset

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸å¤šè·¯å¾„è®¾ç½®
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
tokenizer_path = os.path.join(current_dir, "..", "med_tokenizer_result")

# å®šä¹‰æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
data_dir = os.path.join(current_dir, "..", "dataset")

# å°†ä¸¤ä¸ªæ–‡ä»¶åæ”¾å…¥åˆ—è¡¨ä¸­
data_files = [
    os.path.join(data_dir, "train_encyclopedia.json"),
    os.path.join(data_dir, "medical_book_zh.json")
]

# åŠ è½½ä½ äº²æ‰‹ç»ƒå¥½çš„åˆ†è¯å™¨
tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_path)
tokenizer.pad_token = tokenizer.eos_token

# ==========================================
# 2. å®æ—¶ç”Ÿæˆæµ‹è¯•çš„å›è°ƒç±»
# ==========================================
class VisualProgressCallback(TrainerCallback):
    """
    è¿™ä¸ªç±»ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯éš”ä¸€å®šæ­¥æ•°è¢«è°ƒç”¨ï¼Œ
    è®©æ¨¡å‹å°è¯•å†™ä¸€æ®µè¯ï¼Œå±•ç¤ºå…¶å­¦ä¹ è¿›åº¦ã€‚
    """
    def on_log(self, args, state, control, model=None, **kwargs):
        if state.global_step > 0:
            print(f"\n\n--- ğŸ¤– è®­ç»ƒæ­¥æ•° ç¬¬ {state.global_step} æ­¥çš„æ¨¡å‹è¯•è¿è¡Œ ---")
            prompt = "æˆ‘ä»Šå¤©æœ‰ç‚¹å¤´ç–¼ï¼Œæˆ‘éœ€è¦"
            
            # å°†æç¤ºè¯è½¬æ¢ä¸ºæ•°å­—ï¼ˆToken IDsï¼‰
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # æ¨¡å‹å°è¯•ç”Ÿæˆæ–‡æœ¬
            model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=100,      # ç”Ÿæˆ 100 ä¸ªå­—ä»¥å†…
                    do_sample=True,          # é‡‡æ ·æ¨¡å¼ï¼Œå¢åŠ å¤šæ ·æ€§
                    top_k=50, 
                    top_p=0.95,
                    temperature=0.8,         # è¶Šä½è¶Šä¿å®ˆï¼Œè¶Šé«˜è¶Šæœ‰åˆ›é€ åŠ›
                    pad_token_id=tokenizer.eos_token_id
                )
            model.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
            
            decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼š\n{decoded_text}")
            print("-" * 50 + "\n")

# ==========================================
# 3. åˆå§‹åŒ–æ¨¡å‹ç»“æ„ (Baby-LLM)
# ==========================================
config = GPT2Config(
    vocab_size=len(tokenizer),
    n_embd=256,
    n_layer=4, 
    n_head=8,
    n_positions=512
)
model = GPT2LMHeadModel(config)
print(f"âœ… æ¨¡å‹ç»“æ„å·²å»ºç«‹ï¼Œè¯è¡¨å¤§å°: {len(tokenizer)}ï¼Œæ€»å‚æ•°é‡: {model.num_parameters() / 1e6:.2f} M")

# ==========================================
# 4. æ•°æ®å¤„ç†æµæ°´çº¿
# ==========================================
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

# åŠ è½½æ•°æ®
print("æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™...")
raw_dataset = load_dataset("json", data_files=data_files, split="train")
tokenized_dataset = raw_dataset.map(
    tokenize_function, 
    batched=True, 
    num_proc=4, # åˆ©ç”¨ Mac å¤šæ ¸
    remove_columns=["text"]
)

# æ•°æ®é›†æ•´ç†å™¨
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# ==========================================
# 5. è®­ç»ƒå‚æ•°ä¸å¯åŠ¨
# ==========================================
training_args = TrainingArguments(
    output_dir="./med_model_checkpoints",
    num_train_epochs=1,
    per_device_train_batch_size=4,   # Mac 32Gå†…å­˜ï¼Œ4æ¯”è¾ƒç¨³
    save_steps=500,
    save_total_limit=2,
    logging_steps=50,                # æ¯ 50 æ­¥è¿›è¡Œä¸€æ¬¡æ—¥å¿—è®°å½•ï¼ˆå¹¶è§¦å‘æ¨¡å‹è¯•è¿è¡Œï¼‰
    learning_rate=5e-4,
    weight_decay=0.01,
    fp16=False,                      # Intel Mac å¿…é¡»è®¾ä¸º False
    push_to_hub=False,
    report_to="none"                 # æš‚æ—¶ä¸ä¸Šä¼ æ—¥å¿—
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
    callbacks=[VisualProgressCallback()] # æ³¨å…¥æˆ‘ä»¬å†™çš„å®æ—¶æ˜¾ç¤ºæ’ä»¶
)

print("ğŸš€ é¢„è®­ç»ƒå³å°†å¼€å§‹ã€‚è¯·å…³æ³¨æ§åˆ¶å°ï¼Œæ¯ 50 æ­¥æ¨¡å‹ä¼šä¸ºæ‚¨å†™ä¸€æ®µè¯ã€‚")
# ä¸‹é¢è¿™æ®µä»£ç æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶éœ€æ‰§è¡Œçš„
# trainer.train()

# ä¸­é€”åœæ­¢é‡æ–°å¼€å§‹è®­ç»ƒæ—¶ä¸éœ€è¦é‡æ–°å¼€å§‹ï¼Œè€Œæ˜¯æ‰¾åˆ°ä½ æ–‡ä»¶å¤¹é‡Œç¼–å·æœ€å¤§çš„é‚£ä¸ªï¼Œæ¯”å¦‚ checkpoint-2000
trainer.train(resume_from_checkpoint="./med_model_checkpoints/checkpoint-2000")

# ä¿å­˜æœ€ç»ˆç‰ˆæœ¬
model.save_pretrained("./final_med_model")
tokenizer.save_pretrained("./final_med_model")
print("â­ æ­å–œï¼æ¨¡å‹å·²ç‚¼æˆã€‚")