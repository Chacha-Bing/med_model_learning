import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

# 1. è®¾ç½®è·¯å¾„ï¼ˆæŒ‡å‘ä½ æœ€åç”Ÿæˆçš„é‚£ä¸ªæ–‡ä»¶å¤¹ï¼‰
# model_path = "./med_model_checkpoints/checkpoint-92474" 
model_path = "./base_med_model" 

print("æ­£åœ¨å”¤é†’åŒ»ç”Ÿï¼Œè¯·ç¨å€™...")

# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# ç¡®ä¿ pad_token è®¾ç½®æ­£ç¡®
tokenizer.pad_token = tokenizer.eos_token

# 3. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œå¹¶å…³é—­æ¢¯åº¦è®¡ç®—
model.eval()

def medical_chat():
    print("\n--- ğŸ¥ æ¬¢è¿æ¥åˆ° AI åŒ»ç–—å’¨è¯¢å®¤ (é¢„è®­ç»ƒåŸºåº§ç‰ˆ) ---")
    print("æç¤ºï¼šå½“å‰æ¨¡å‹ä»…å®Œæˆé¢„è®­ç»ƒï¼Œå®ƒä¼šå°è¯•ã€ç»­å†™ã€ä½ çš„è¯ã€‚è¾“å…¥ 'quit' é€€å‡ºã€‚")
    
    while True:
        user_input = input("\nğŸ§ ä½ æƒ³å’¨è¯¢ä»€ä¹ˆï¼Ÿï¼š")
        if user_input.lower() == 'quit':
            break
            
        # æ„é€ è¾“å…¥
        inputs = tokenizer(user_input, return_tensors="pt")
        
        # 4. ç”Ÿæˆç­”æ¡ˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,      # é™åˆ¶é•¿åº¦
                do_sample=True,          # å¼€å¯é‡‡æ ·
                top_p=0.9,               # æ ¸é‡‡æ ·ï¼Œè¿‡æ»¤ä½æ¦‚ç‡è¯
                temperature=0.7,         # æ§åˆ¶éšæœºæ€§ï¼Œ0.7 æ¯”è¾ƒç¨³å¥
                repetition_penalty=1.2,  # é‡ç‚¹ï¼å¢åŠ æƒ©ç½šï¼Œå‡å°‘â€œé‡å¤â€ç°è±¡
                pad_token_id=tokenizer.eos_token_id
            )
            
        # 5. è§£ç å¹¶æ˜¾ç¤º
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nğŸ‘¨â€âš•ï¸ åŒ»ç”Ÿå»ºè®®ï¼š\n{result}")

if __name__ == "__main__":
    medical_chat()