# è¿™ä¸ªè„šæœ¬åŠ è½½ä¸åŒæ­¥éª¤ç”Ÿæˆåçš„å…±è®¡3ä¸ªæ¨¡å‹ï¼šé¢„è®­ç»ƒç”Ÿæˆåçš„æ¨¡å‹ã€SFTåç”Ÿæˆçš„æ¨¡å‹ã€DPOåç”Ÿæˆçš„æ¨¡å‹ï¼Œåªéœ€è¦åœ¨å‘½ä»¤è¡Œæ‰“ä¸€ä¸ªé—®é¢˜ï¼Œä¸‹é¢è‡ªåŠ¨å‡ºç°ä¸‰ä¸ªæ¨¡å‹ç”Ÿæˆçš„å›å¤ä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒ
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import time

# 1. è·¯å¾„é…ç½®ï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…æ–‡ä»¶å¤¹åä¿®æ”¹ï¼‰
models = {
    "Base (é€šè¿‡é¢„è®­ç»ƒ)": "./base_model__after_pretraining", 
    "SFT (é€šè¿‡æŒ‡ä»¤å¾®è°ƒ)": "./post_model__after_sft",
    "DPO (é€šè¿‡åå¥½å¯¹é½)": "./final_model__after_dpo" # ä½¿ç”¨ä½ æ•ˆæœæœ€å¥½çš„é‚£ä¸ª
}

def generate_response(model_path, prompt, tokenizer):
    print(f"\n[æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path} ...]")
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()
    
    # æ„å»ºå¯¹è¯æ¨¡ç‰ˆ (ç¡®ä¿ä¸ SFT/DPO è®­ç»ƒæ—¶ä¸€è‡´)
    full_prompt = f"é—®ï¼š{prompt} ç­”ï¼š"
    inputs = tokenizer(full_prompt, return_tensors="pt")
    
    # å¼€å§‹ç”Ÿæˆ
    start_time = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,        # æ”¾å®½åˆ° 500 tokens
            do_sample=True,
            temperature=0.7,           # ä¿æŒé€‚åº¦éšæœºæ€§
            top_p=0.9,
            repetition_penalty=1.2,    # ç¨å¾®åŠ å¤§æƒ©ç½šï¼Œç¼“è§£å¤è¯»
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    
    duration = time.time() - start_time
    # è§£ç å¹¶æˆªæ–­ prompt éƒ¨åˆ†
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_text.replace(full_prompt, "").strip()
    
    # é‡Šæ”¾æ¨¡å‹å†…å­˜ï¼Œé˜²æ­¢ Mac å¡æ­»
    del model
    return response, duration

def main():
    # åªéœ€è¦åŠ è½½ä¸€æ¬¡åˆ†è¯å™¨
    tokenizer = GPT2TokenizerFast.from_pretrained(models["SFT (é€šè¿‡æŒ‡ä»¤å¾®è°ƒ)"])
    tokenizer.pad_token = tokenizer.eos_token

    print("="*50)
    print("ğŸ¥ åŒ»ç–—å°æ¨¡å‹è¿›åŒ–å¯¹æ¯” ğŸ¥")
    print("="*50)

    while True:
        user_input = input("\nè¯·è¾“å…¥åŒ»å­¦é—®é¢˜ (è¾“å…¥ q é€€å‡º): ")
        if user_input.lower() == 'q':
            break

        results = {}
        for name, path in models.items():
            try:
                response, dt = generate_response(path, user_input, tokenizer)
                results[name] = (response, dt)
            except Exception as e:
                results[name] = (f"åŠ è½½å¤±è´¥: {str(e)}", 0)

        # æœ€ç»ˆåŒå±è¾“å‡ºå¯¹æ¯”
        print("\n" + "âœ¨" * 25)
        for name, (resp, dt) in results.items():
            print(f"\nã€{name}ã€‘ (è€—æ—¶: {dt:.2f}s):")
            print("-" * 30)
            print(resp if resp else "[æ¨¡å‹æœªè¾“å‡ºå†…å®¹]")
            print("-" * 30)
        print("âœ¨" * 25)

if __name__ == "__main__":
    main()