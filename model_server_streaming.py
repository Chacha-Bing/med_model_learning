import torch
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TextIteratorStreamer
from threading import Thread # å¿…é¡»ä½¿ç”¨çº¿ç¨‹æ¥å¤„ç†æµ
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰æ¥æº
    allow_methods=["*"],
    allow_headers=["*"],
)

MODELSTYLE = {
  "BASE": "Base (é€šè¿‡é¢„è®­ç»ƒ)",
  "SFT": "SFT (é€šè¿‡æŒ‡ä»¤å¾®è°ƒ)",
  "DPO": "DPO (é€šè¿‡åå¥½å¯¹é½)"
}

models = {
    MODELSTYLE["BASE"]: "./base_model__after_pretraining", 
    MODELSTYLE["SFT"]: "./post_model__after_sft",
    MODELSTYLE["DPO"]: "./final_model__after_dpo" # ä½¿ç”¨ä½ æ•ˆæœæœ€å¥½çš„é‚£ä¸ª
}

async def stream_generate(model_path, prompt, tokenizer):
    # 1. åŠ è½½æ¨¡å‹ (å»ºè®®ç”Ÿäº§ç¯å¢ƒé¢„åŠ è½½ï¼Œä¸è¦æ¯æ¬¡è¯·æ±‚éƒ½åŠ è½½)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()

    full_prompt = f"é—®ï¼š{prompt} ç­”ï¼š"
    inputs = tokenizer(full_prompt, return_tensors="pt")

    # 2. åˆå§‹åŒ– Streamer
    # skip_prompt=True ä¼šè·³è¿‡è¾“å…¥çš„â€œé—®ï¼š... ç­”ï¼šâ€éƒ¨åˆ†ï¼Œåªæµå‡º AI å›å¤çš„éƒ¨åˆ†
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # 3. æ„å»ºç”Ÿæˆå‚æ•°
    generation_kwargs = dict(
        **inputs,
        streamer=streamer, # æ ¸å¿ƒï¼šå°† streamer ä¼ å…¥
        max_new_tokens=500,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    # 4. åœ¨å­çº¿ç¨‹ä¸­å¯åŠ¨æ¨¡å‹ç”Ÿæˆ
    # ä¸ºä»€ä¹ˆï¼Ÿå› ä¸º generate æ˜¯é˜»å¡çš„ï¼Œå¦‚æœä¸æ”¾è¿›çº¿ç¨‹ï¼Œä¸»çº¿ç¨‹å°±æ²¡æ³• yield æ•°æ®äº†
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # 5. ä» streamer ä¸­é€ä¸ªè¿­ä»£ç”Ÿæˆçš„æ–‡å­—
    for new_text in streamer:
        yield new_text # çœŸæ­£çš„æµå¼è¾“å‡º
        # ä¸éœ€è¦æ‰‹åŠ¨ sleepï¼Œæ¨¡å‹ç”Ÿæˆçš„å¿«æ…¢å†³å®šäº†æµçš„é€Ÿåº¦

    # é‡Šæ”¾æ˜¾å­˜/å†…å­˜
    del model

class ChatRequest(BaseModel):
    prompt: str
    model_called: Optional[str] = models[MODELSTYLE["BASE"]]

@app.post("/generate")
async def generate(request: ChatRequest):
    # åªéœ€è¦åŠ è½½ä¸€æ¬¡åˆ†è¯å™¨
    tokenizer = GPT2TokenizerFast.from_pretrained(models[MODELSTYLE["SFT"]])
    tokenizer.pad_token = tokenizer.eos_token
    
    print(f"ğŸ¥ åŒ»ç–—å°æ¨¡å‹æµå¼æ¨ç†å¼€å§‹: {request.model_called}")

    # è¿”å›æµå¼å“åº”
    return StreamingResponse(
        stream_generate(request.model_called, request.prompt, tokenizer),
        media_type="text/plain"
    )
  
  
if __name__ == "__main__":
  import uvicorn
  # æ‰“å°ä¸€å¥è¯ï¼Œç¡®ä¿ä½ çœ‹åˆ°å®ƒå¼€å§‹äº†
  print("ğŸš€ åŒ»ç–— AI æ¨¡å‹æ¨ç†æœåŠ¡æ­£åœ¨å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 8000...")
  uvicorn.run(app, host="0.0.0.0", port=8000)